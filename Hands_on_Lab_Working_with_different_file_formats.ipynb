{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvuUZssNct4m7r6OASknX5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matinnorouzi2023/data-science/blob/main/Hands_on_Lab_Working_with_different_file_formats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Lab: Working with different file formats\n",
        "\n",
        "Estimated time: **40 mins**"
      ],
      "metadata": {
        "id": "Cw9b3j4vIFQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "\n",
        "1.  <a href=\"#Data-Engineering\">Data Engineering</a>\n",
        "2.  <a href=\"#Data-Engineering-Process\">Data Engineering Process</a>\n",
        "3.  <a href=\"#Working-with-different-file-formats\">Working with different file formats</a>\n",
        "4.  <a href=\"#Data-Analysis\">Data Analysis</a>\n",
        "\n",
        "</font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "u-2inpJBIOPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineering"
      ],
      "metadata": {
        "id": "-XQVHrFeIOXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data engineering** is one of the most critical and foundational skills in any data scientist’s toolkit."
      ],
      "metadata": {
        "id": "NAYQFgLNISda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I35q5bHgIVce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineering Process"
      ],
      "metadata": {
        "id": "PP29rCZRIViV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several steps in Data Engineering process.\n",
        "\n",
        "1.  **Extract** - Data extraction is getting data from multiple sources. Ex. Data extraction from a website using Web scraping or gathering information from the data that are stored in different formats(JSON, CSV, XLSX etc.).\n",
        "\n",
        "2.  **Transform** - Transforming the data means removing the data that we don't need for further analysis and converting the data in the format that all the data from the multiple sources is in the same format.\n",
        "\n",
        "3.  **Load** - Loading the data inside a data warehouse. Data warehouse essentially contains large volumes of data that are accessed to gather insights."
      ],
      "metadata": {
        "id": "HRK_AHR0IYmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with different file formats"
      ],
      "metadata": {
        "id": "uig-rga-IeBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the real-world, people rarely get neat tabular data. Thus, it is mandatory for any data scientist (or data engineer) to be aware of different file formats, common challenges in handling them and the best, most efficient ways to handle this data in real life. We have reviewed some of this content in other modules."
      ],
      "metadata": {
        "id": "yH9-HbAYIjOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### File Format"
      ],
      "metadata": {
        "id": "IbJdH4-lIoXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>A file format is a standard way in which information is encoded for storage in a file. First, the file format specifies whether the file is a binary or ASCII file. Second, it shows how the information is organized. For example, the comma-separated values (CSV) file format stores tabular data in plain text.\n",
        "\n",
        "To identify a file format, you can usually look at the file extension to get an idea. For example, a file saved with name \"Data\" in \"CSV\" format will appear as **Data.csv**. By noticing the **.csv** extension, we can clearly identify that it is a **CSV** file and the data is stored in a tabular format.</p>"
      ],
      "metadata": {
        "id": "ACG5xoMVIoai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various formats for a dataset, .csv, .json, .xlsx etc. The dataset can be stored in different places, on your local machine or sometimes online.\n",
        "\n",
        "**In this section, you will learn how to load a dataset into our Jupyter Notebook.**\n"
      ],
      "metadata": {
        "id": "Pgn8xYI3Iv0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will look at some file formats and how to read them in Python:"
      ],
      "metadata": {
        "id": "Ynm81R69Ix9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comma-separated values (CSV) file format"
      ],
      "metadata": {
        "id": "CKTf9eBRI0lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Comma-separated values** file format falls under a spreadsheet file format.\n",
        "\n",
        "In a spreadsheet file format, data is stored in cells. Each cell is organized in rows and columns. A column in the spreadsheet file can have different types. For example, a column can be of string type, a date type, or an integer type.\n",
        "\n",
        "Each line in CSV file represents an observation, or commonly called a record. Each record may contain one or more fields which are separated by a comma.\n"
      ],
      "metadata": {
        "id": "vTCvuT_cI3x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading data from CSV in Python"
      ],
      "metadata": {
        "id": "bK8f0zAzI5_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Pandas** Library is a useful tool that enables us to read various datasets into a Pandas data frame\n",
        "\n",
        "Let us look at how to read a CSV file in Pandas Library.\n",
        "\n",
        "We use **pandas.read_csv()** function to read the csv file. In the parentheses, we put the file path along with a quotation mark as an argument, so that pandas will read the file into a data frame from that address. The file path can be either a URL or your local file address."
      ],
      "metadata": {
        "id": "6ZUDqIvCI72E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import piplite\n",
        "await piplite.install(['seaborn', 'lxml', 'openpyxl'])\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UEH4zVTNJFVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyodide.http import pyfetch\n",
        "\n",
        "filename = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/labs/Module%205/data/addresses.csv\"\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())\n",
        "\n",
        "await download(filename, \"addresses.csv\")\n",
        "\n",
        "df = pd.read_csv(\"addresses.csv\", header=None)"
      ],
      "metadata": {
        "id": "RjEmQpg7JHrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43aaIa-FH8YY"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding column name to the DataFrame\n",
        "\n",
        "We can add columns to an existing DataFrame using its **columns** attribute."
      ],
      "metadata": {
        "id": "3UBW2TW-JKs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding column name to the DataFrame\n",
        "\n",
        "We can add columns to an existing DataFrame using its **columns** attribute."
      ],
      "metadata": {
        "id": "FvG-l0-iJav0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns =['First Name', 'Last Name', 'Location ', 'City','State','Area Code']"
      ],
      "metadata": {
        "id": "kWaIDGiDJekm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Q1m6uqkDJS9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selecting a single column\n",
        "\n",
        "To select the first column 'First Name', you can pass the column name as a string to the indexing operator."
      ],
      "metadata": {
        "id": "XcVHiMqDJjlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['First Name', 'Last Name', 'Location ', 'City','State','Area Code']]\n",
        "df"
      ],
      "metadata": {
        "id": "pmKxCSbgJlsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selecting rows using .iloc and .loc\n",
        "\n",
        "Now, let's see how to use .loc for selecting rows from our DataFrame.\n",
        "\n",
        "**loc() : loc() is label based data selecting method which means that we have to pass the name of the row or column which we want to select.**"
      ],
      "metadata": {
        "id": "AKVJ4EzDJu7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To select the first row\n",
        "df.loc[0]"
      ],
      "metadata": {
        "id": "UbP7wG33Jw4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
        "df.loc[[0,1,2], \"First Name\" ]"
      ],
      "metadata": {
        "id": "IlyVTAGJJycX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see how to use .iloc for selecting rows from our DataFrame.\n",
        "\n",
        "**iloc() : iloc() is a indexed based selecting method which means that we have to pass integer index in the method to select specific row/column.**"
      ],
      "metadata": {
        "id": "fq-vOW-AJy3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To select the 0th,1st and 2nd row of \"First Name\" column only\n",
        "df.iloc[[0,1,2], 0]"
      ],
      "metadata": {
        "id": "egCFrk3HJ1Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information please read the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01).\n",
        "\n",
        "Let's perform some basic transformation in pandas."
      ],
      "metadata": {
        "id": "uSOBjBToJ6qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Function in Pandas\n",
        "\n",
        "Python's Transform function returns a self-produced dataframe with transformed values after applying the function specified in its parameter.\n",
        "\n",
        "Let's see how Transform function works."
      ],
      "metadata": {
        "id": "N-jzjjrBKDeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import library\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JQaqlqLcJ_Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a dataframe\n",
        "df=pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
        "df"
      ],
      "metadata": {
        "id": "jTIoJuNyKFoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s say we want to add 10 to each element in a dataframe:"
      ],
      "metadata": {
        "id": "XTGpVuShKJn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#applying the transform function\n",
        "df = df.transform(func = lambda x : x + 10)\n",
        "df"
      ],
      "metadata": {
        "id": "KG1QyZqmKLLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use DataFrame.transform() function to find the square root to each element of the dataframe."
      ],
      "metadata": {
        "id": "azbiZmcAKNSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = df.transform(func = ['sqrt'])"
      ],
      "metadata": {
        "id": "hXXkBw9GKPTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "kRFzCWPQKQ4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information about the **transform()** function  please read the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01)."
      ],
      "metadata": {
        "id": "zsHkyxynKSzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON file Format"
      ],
      "metadata": {
        "id": "Oe-E0aNiKV48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JSON (JavaScript Object Notation)** is a lightweight data-interchange format. It is easy for humans to read and write.\n",
        "\n",
        "JSON is built on two structures:\n",
        "\n",
        "1.  A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.\n",
        "\n",
        "2.  An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.\n",
        "\n",
        "JSON is a language-independent data format. It was derived from JavaScript, but many modern programming languages include code to generate and parse JSON-format data. It is a very common data format with a diverse range of applications."
      ],
      "metadata": {
        "id": "FoHu30rnKYY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text in JSON is done through quoted string which contains the values in key-value mappings within { }. It is similar to the dictionary in Python."
      ],
      "metadata": {
        "id": "ADF8tbcFKau2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python supports JSON through a built-in package called **json**. To use this feature, we import the json package in Python script."
      ],
      "metadata": {
        "id": "ioU_kPrQKbJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "2_P8M9NbKebr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}